<!DOCTYPE html>

<html><head><link href="http://jonoflayham.com/blog/index.html" rel="canonical" /><meta content="True" name="HandheldFriendly" /><meta content="320" name="MobileOptimized" /><meta content="width=device-width, initial-scale=1.0" name="viewport" /><meta charset="utf-8" /><title>Loose Typing</title><link href="/css/dcd1ecc6fd00/theme.css" rel="stylesheet" /><link href="/css/d4de3943cb2d/zenburn-custom.css" rel="stylesheet" /><link href="http://fonts.googleapis.com/css?family=Poller+One" rel="stylesheet" type="text/css" /><link href="http://fonts.googleapis.com/css?family=Germania+One" rel="stylesheet" type="text/css" /><link href="http://fonts.googleapis.com/css?family=Fontdiner+Swanky" rel="stylesheet" type="text/css" /><link href="http://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" /><link href="http://fonts.googleapis.com/css?family=Cardo" rel="stylesheet" type="text/css" /></head><body><header role="banner"><hgroup><h1><a href="/">Loose Typing</a></h1></hgroup></header><nav role="navigation"><ul class="main-navigation"><li><a href="/">Blog</a></li><li><a href="/archive/">Archive</a></li><li><a href="/atom.xml">RSS</a></li><li><a href="http://twitter.com/jonoflayham">Twitter</a></li><li><a href="https://github.com/jonoflayham">Github</a></li></ul></nav><div id="main"><div id="content"><div class="blog-index"><article><header><h1 class="entry-title"><a href="/blog/2017/02/04/make-your-own-tools/">Make your own tools</a></h1><p class="meta"><time datetime="2017-02-04T00:00:00.000Z">04 Feb 2017</time></p></header><div class="body entry-content"><p>As developers, I wish we dared more to step away from the blunt, primitive tools at the bottom of the software development stack - IDEs, logs, CI servers, the command line - to write higher-order tools that can help us work more effectively and give us more enjoyment in our jobs. We should relish using our skills to make our lives easier and deliver better software. Too often we fail to do this, and we're missing out on real opportunities to improve.</p><p>By "tools", I mean useful stuff accessed from the command line or REPL, or from within a dedicated app, or exposed through a REST interface, or accessed through a GUI - whatever control and view surfaces make sense for the way in which the facility needs to be used.</p><p>It's odd, really. When we're writing code that's more than few lines long, we soon think of breaking it up into chunks and using those chunks at a higher level, then as things grow still more we refactor <em>that</em> and compose over it, and so on up the ladder of abstraction. We don't end up writing files of source code thousands of lines long, or at least I hope not. But when it comes to the activities we code ourselves to do, whether at the computer or in our interactions with each other and with people outside the team, we're perfectly happy bolting together the same old steps. We seem unaware that we're death-marching through numerous repetitive, un-factored-out, long-winded, ambiguously-defined, error-prone steps time and time again. Click, click, copy, browse, copy, type, email, chat, type, type, Slack, chat, click, type... "Oh, wait! Why don't I write this up on a wiki page?"</p><p>There are many reasons why we tend not to write our own tools, or tend not to give much love to the ones we do create.</p>
<ol>
  <li><p>We don't always need much more than what we already have, so we allow ourselves to be drugged by familiarity and forget that it's possible to improve.</p></li>
  <li><p>When we're not so experienced, we take it as read that everything's been done for us. "Surely Maven is all you need!"</p></li>
  <li><p>It's quite easy to write a wiki page. You get to be a published author! Your effort is evident to others, and they'll thank you for it. And when it comes to using it, it's actually quite comforting to follow instructions. For the first or second time, at least.</p></li>
  <li><p>We might realise we need to make better tools, but developing them is sometimes enough of an overhead to be unmotivating.</p></li>
  <li><p>Sometimes automation and tooling hide things which it's useful to see, especially at the start of a project or a new kind of activity. That's why manual testing is still valuable. But it means we get used to not automating stuff.</p></li>
  <li><p>When we're working at scales where the effort definitely would be worth our while, there's often too much weight of opinion behind old practices, and we're engaged in so much fire-fighting - ironically often exacerbated by the very lack of tooling - that we can't think clearly in any case.</p></li>
  <li><p>Our work might be controlled by someone who describes themselves as "delivery-focussed" and hasn't experienced the value of taking stock and investing in work which is at one remove from the increments of business value they want us to get over the line. To someone in that position, a team writing its own tools is a team wasting time.</p></li>
  <li><p>Writing tools to solve messy software development problems often involves understanding many kinds of technologies, and not everyone has the confidence or the breadth of knowledge required.</p></li>
  <li><p>There might be a Product out there already which Already Does This. The people setting the agenda will see writing something yourself as unnecessary effort, even if there's every chance the enterprise equivalent is expensive, bloated and doesn't in practice do what you need, or at least not without considerable customisation. And money. And formal procedures for requesting installation. And teams of experts. And time spent finding nothing useful about it on StackOverflow. And restrictions born of the fact that it's shared by many people. And downtime.</p></li>
  <li><p>Larger companies, especially, tend to mandate the use of specific toolsets, and explicitly making more tools is naughty. "We already have everything we need!" Which will either be nothing or one of those bloated enterprise things.</p></li>
  <li><p>Although we might be highly motivated to improve our lot, that motivation might be bound up with to wanting to make a real difference quickly - maybe because we fear it has to be done under the radar - and we may feel we can best do that in a language which fires us up but isn't that of the production system nor of other developers in the team. Once again, this is deemed naughty. It is apparently far better to have nothing than to have something that might take a bit of effort to maintain, and apparently it's madness to expect programmers to understand more than one language. (Don't get me started on the destructive 10,000 hours meme.) I've seen hugely valuable tools start life out in hack days, where people are given their freedom.</p></li>
  <li><p>Conversely, when it comes to integrating the kinds of tools we deal with as developers, we're often suckered into believing we can't use the first class programming languages we know and love - even if they're the same ones we're using to develop the functionality of our production system.</p></li>
  <li><p>When we're done writing our tools, there might not be anywhere acceptable to host or distribute whatever we've written, and so it won't get used and can't be promoted.</p></li>
  <li><p>Finally, there are those of us who give the whole 'invent what you need' principle a bad name because in the past we've applied it for the wrong reasons, preferring to solve an easier, more interesting and mostly unrelated problem rather than the one our clients care about and are paying us for.</p></li>
</ol><p>So here's an example. It's about a particular idea that's been knocking around in my head for ages, one that would have been of real utility in several widely varying contexts, but that I've not developed anywhere except in my head. I mention it now for a couple of reasons. Firstly, it's an example of a failure on my part to apply the principle I'm talking about here, for most of the reasons above. Secondly, the value of the idea is screaming at me in the current day job, and I don't want to make the same mistake by ignoring that. </p><p>The idea is about a little toolset for using test data in complex enterprise systems. It's an idea that starts life really simple. It's easy to begin. But it fertilises <a href="https://www.youtube.com/watch?v=OFzXaFbxDcM">the garden of your mind</a> for other ideas to grow - again, we're climbing up the ladder of abstraction. As well as wildly mixing metaphors.</p><p>Nowhere do we need better tools for testing software than in large-scale enterprises. Like it or not, delivering software in big organisations usually means dealing with real legacy systems in unreliable environments which contain (or serve up) test data of deeply uncertain provenance and all-too-evident unreliability. Test data is often won at significant cost, perhaps involving whole teams of people to mine for it or create if from scratch. It's jealously hoarded once acquired, but rarely safe from interference. It's often intermittently available as the environments which deliver it come and go.</p><p>Despite its problems, realistic test data is invaluable in proving the behaviour of complex systems that have parts not under your control - the parts that a friend recently labelled "here be dragons". You'd be a brave or foolish person to take new functionality live without making use of test data and the systems doling it out.</p><p>Large-scale enterprises commonly have dedicated test teams - QAs - and it's the QAs who understand, discover and acquire test data. You might scoff at the idea of a separate QA team, but I'll wager that you'll be happy for them to take on all that messy work! Many is the situation in which my team would have been lost without QAs. Their knowledge and understanding is gold dust, and they're not afraid of panning through mud and silt to get to it.</p><p>But information about test data is usually disseminated pretty poorly. It's emailed all over the place, for a start. If you're lucky it will appear in as a spreadsheet attachment. If you're luckier still it will be shared in a wiki page, though usually still as an attachment. Someone might <em>even</em> go so far as to make it more readily machine-readable.</p><p>And that's usually as far as it goes. It's rarely <em>actually</em> machine-read. The association of your tests with the test data they need is all manual. This is understandable when the testing is manual. But even in the nicest of automated tests which rely on that data, expressed in the most sensible way, there's still a huge gap between saying "Given I am a customer with a Volkswagen" and choosing account number 234292325 under the assumption that it really does illustrate that scenario. That gap's filled by the person writing the test, who reads the wiki page or whatever then types "234292325" into their Cucumber or plain-code test, where it's trusted forever afterwards.</p><p>To keep ourselves honest, we'd need a sub-test embedded in the Given step, the test precondition: namely, an assertion that 234292325 really is such an account. It's rare in big systems to be able to verify this directly, so typically the best we could do is to assert that 234292325 is mentioned on the test data wiki page in the section marked "user accounts who have Volkswagens". Then we'd know that if the QA was right, we're good to go. But even that is usually too much work, so we give up completely and just type in "234292325", hoping that no-one screws with that account and that the QA <em>was</em> right and that the environment containing it stays up, and so perpetuate the hiding of our assumptions.</p><p>If someone screws with the account, or the QA was wrong, or something in the way it was acquired or created was wrong, then you may eventually discover the fact. Assuming, that is, that your test does actually fail and isn't passing coincidentally. If your test does fail, you'll grub around for another account number to use, if you can remember where the wiki page or the spreadsheet is, all the while hoping that the list of test data is up to date and the data is still truly available in the environment which allegedly used to contain it.</p><p>Worse still, you may well have brought into question the provenance of some carefully curated fossil of test data - but no-one else will know about your discovery, and will carry on wasting their time with it. That may even be you, because you'll have used the same precious artefact in several tests, and several kinds of tests, and might not remember to search-and-replace.</p><p>What if there were a place to describe test data and the things about it that are of interest to your team and people working in your domain, and what if your tests had first-class, no-air-gap access to that kind of material? What if you didn't have to grub around for documentation about what test data is available in which environment with which characteristics, and instead could just ask for it? What if you could actually ask for it in code at test run time? What if you could automate tests over all allegedly-matching pieces of test data? What if there were an effective, simple, shared place where the facts about the test data we rely upon could be maintained?</p><p>Some of us might need to challenge our assumption that tests should always be repeatable. In the sense people usually mean it, that's usually rot. It's not of much value to have proven only that your e-commerce site definitely works when you log in as one customer, <a href="mailto:customer@gmail.com">customer@gmail.com</a>. If you use data x to verify function y, and x never changes, then you might fail to spot that data z breaks your test - isn't that something you'd like to know? As long we could be sure we were getting only data of the kind we needed, there'd be much greater value in our tests if they exercised scenarios using all of that data, however much becomes available at various times. After all, we're trying to verify the system's behaviour in the face of variance in realistic data. The repeatability which we're <em>really</em> seeking is the ability to be able to re-apply the same test to the same dynamically chosen data which caused it to fail the last time: think generative testing but with the preconditions instead of the stimuli being the invariants. If we managed test data properly, and integrated it properly with our test suites, we can begin to see how we might do this.</p><p>With something like this in place, everyone could benefit at once. Once you'd discovered something about a bit of data - eg that it didn't after all correspond to a customer who'd said "yes" to marcomms - you'd bank that bit of knowledge, and you and everyone would benefit thereafter, and avoid wrongly using it in proving behaviour in corresponding scenarios.</p><p>We can get higher-order pretty quickly. With a declarative description of what you're interested in, you could use it to <em>create</em> test data, for a certain value of 'create'. Creation might mean automatically spitting out the specification of the test data you want, then emailing that to your manual test data creation team. It might mean going off, with privileged access to the databases that are otherwise invisible to the system you're developing, and which are read-only for you and your team, to <em>find</em> the data and highlight what's missing: "there are no customers in the system test environment who've bought a Volkswagen - this needs fixing, please". Once you've found data meeting your expectations, you could assert its viability for everyone to take advantage of, thereby making it useful for any tests declaring their reliance on matching preconditions.</p><p>We're not dealing here only with the messiness of acquiring realistic test data and confirming its realism. Effective testing of this kind also means squarely facing up to its instability, to the fact that bits of it are liable to change under your feet at any time. Time is fact the key. We need to have a first-class notion of the time at which facts are true, because that will help us understand test failures and attribute them to the right cause. Wouldn't it be great never again to have to undergo the kind of dismaying, futile archaeology which so often results when things go wrong and is the more time-consuming and exhausting the more people and the more teams involved? "Well, it worked yesterday in system test but not today, and we've got to go live tomorrow, and they're saying they haven't changed anything." We need our testing infrastructure to be able to time-travel, and to return to the present with answers.</p><p>Once you start thinking like this, the ideas pile on thick and fast. In a way, that's another reason why I've failed in the past to take the first step, because by the time I steel myself up to it, the first step looks decidedly drab compared to the grandiose vision in my head, and it all seems unachievable. I find I can't start off with something like <a href="http://worrydream.com/">Brett Victor</a>'s <a href="https://vimeo.com/36579366">round-tripping</a> <a href="https://www.youtube.com/watch?v=PUv66718DII">code editor</a>, and I give up then and there.</p><p>So let's come back down to reality and wind back to the starting position vis-à-vis enterprise test data support. The first few steps to giving ourselves a tool to help ought to be really simple. If we give ourselves permission we'll see more and more opportunities for improvement using the skills we already have at our fingertips. And it's that which is really what's most motivating about delivering software: making a useful difference more quickly than last time, getting to powerful and interesting levels of abstraction, and giving yourself more time to think.</p><p>I'm going to step back into the day job to try out this particular idea. I'm also going to reflect on how we can do better in future. A more disciplined approach to reflection - retrospectives, if you like - would help. Finally, I'm going to bear in mind in this and future engagements the stumbling blocks which stand in the way of making improvements like this.</p><p>I'd promise to report back here on how it goes, but given my blogging frequency I might have died before I get round to it.</p></div><div class="pagination"><a href="/archive/">Blog Archive</a></div></article></div></div></div><footer role="contentinfo"><p>Web site copyright © 2017 Jon Woods   |   <span class="credit">Powered by <a href="http://github.com/gilbertw1/blog-gen"> a Little Side Project</a>   |   Mostly themed with <a href="https://github.com/TheChymera/Koenigspress">Königspress</a></span></p></footer></body></html>